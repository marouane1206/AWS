{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f68b87",
   "metadata": {},
   "source": [
    "# Generative AI for Personalized Marketing\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "The code is broken up into cells. In a Jupyter notebook, to run code in a cell, you can click in the cell, and then click the play icon on the toolbar. Or, you can press the [Shift]+[Enter] key combination. When the code cell finishes running, the text to the left of it changes from an asterisk [ * ] to a number.\n",
    "\n",
    "Use the following instructions and run the cells to generate this practice example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782605f",
   "metadata": {},
   "source": [
    "### Items data\n",
    "\n",
    "The items data consists of information about the content that is being interacted with, which generally comes from Content Management Systems (CMS).\n",
    "\n",
    "### Interactions data\n",
    "\n",
    "The ml-latest-small dataset from the [Movielens](https://grouplens.org/datasets/movielens/) project is used as a proxy for user-item interactions. \n",
    "\n",
    "### User data\n",
    "\n",
    "User data is not used in this example to train the Amazon Personalize model because the Movielens dataset does not provide this data. However, you will experiment with different user personas when working on the email preparation and prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125eea3b",
   "metadata": {},
   "source": [
    "## Set up environment\n",
    "\n",
    "#### Cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b8862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import boto3\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e42a6c2",
   "metadata": {},
   "source": [
    "### Prepare the item metadata\n",
    "\n",
    "Load the items data from a CSV file.\n",
    "\n",
    "Note: Your use of IMDb data is for the sole purpose of completing the AWS workshop and/or tutorial. Any use of IMDb data outside of the AWS workshop and/or tutorial requires a data license from IMDb. To obtain a data license, please contact: imdb-licensing-support@imdb.com. You will not (and will not allow a third party to) (i) use IMDb data, or any derivative works thereof, for any purpose; (ii) copy, sublicense, rent, sell, lease or otherwise transfer or distribute IMDb data or any portion thereof to any person or entity for any purpose not permitted within the workshop and/or tutorial; (iii) decompile, disassemble, or otherwise reverse engineer or attempt to reconstruct or discover any source code or underlying ideas or algorithms of IMDb data by any means whatsoever; or (iv) knowingly remove any product identification, copyright or other notices from IMDb data.\n",
    "\n",
    "#### Cell 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e4383",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data = pd.read_csv('imdb/items.csv', sep=',', dtype={'PROMOTION': \"string\"})\n",
    "item_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921705e",
   "metadata": {},
   "source": [
    "#### Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9470ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('imdb/items.csv', sep=',', usecols=[0,1], encoding='latin-1', dtype={'movieId': \"str\", 'imdbId': \"str\", 'tmdbId': \"str\"})\n",
    "pd.set_option('display.max_rows', 25)\n",
    "movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02345da",
   "metadata": {},
   "source": [
    "### Prepare the interactions data\n",
    "\n",
    "Read the interactions data from a CSV file.\n",
    "\n",
    "#### Cell 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a647877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = pd.read_csv('interactions.csv')\n",
    "interactions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c9166",
   "metadata": {},
   "source": [
    "#### Cell 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95661926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique user IDs from the interaction dataset\n",
    "\n",
    "user_ids = interactions_df['USER_ID'].unique()\n",
    "user_data = pd.DataFrame()\n",
    "user_data[\"USER_ID\"]= user_ids\n",
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c653e9b7",
   "metadata": {},
   "source": [
    "## Introduction to Amazon Personalize\n",
    "\n",
    "[Amazon Personalize](https://aws.amazon.com/pm/personalize/) is a fully managed machine learning (ML) service that uses your data to generate item recommendations for your users. Amazon Personalize helps developers build applications with a wide array of personalization use cases, and it automates many of the complicated steps to build, train, and deploy an ML model.  \n",
    "\n",
    "Regardless of the use case, the algorithms all share a base of learning on user-item-interaction data, which is defined by three core attributes:\n",
    "\n",
    "1. **UserID** - The user who interacted\n",
    "1. **ItemID** - The item the user interacted with\n",
    "1. **Timestamp** - The time at which the interaction occurred\n",
    "\n",
    "Generally speaking, your data will not arrive in a perfect form for Amazon Personalize and will take some modification to be structured correctly. This notebook guides you through that process.\n",
    "\n",
    "### Items data\n",
    "\n",
    "The items data consists of information about the content that is being interacted with, which generally comes from Content Management Systems (CMS). For the purpose of this practice example, the IMDb TT ID is used to provide a common identifier between the interactions data and the content metadata. Movielens provides its own identifier as well as a the IMDb TT ID (without the leading 'tt') in the 'links.csv' file. This dataset is not mandatory, but provided good item metadata will ensure the best results in your trained models.\n",
    "\n",
    "### Interactions data\n",
    "\n",
    "The interactions data consists of information about the interactions that the users of the fictional app will have with the content. This usually comes from analytics tools or customer data platforms (CDPs). The best interactions data for use with Amazon Personalize includes the sequential order of user behavior, what content was watched/clicked on, and the order it was interacted with. To simulate our interactions data, data from the [MovieLens project](https://grouplens.org/datasets/movielens/) is used. Movielens offers multiple versions of their dataset, and for the purposes of this practice example, a reduced version of this dataset (approx 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users) is used.\n",
    "\n",
    "### User data\n",
    "\n",
    "The user data is what information you have about your users. It usually comes from customer relationship management (CRM) or subscriber management systems. Because no user data is included in the MovieLens data, a small synthetic dataset will be generated to simulate this component of the practice example. This dataset is not manatory, but provided good user metadata will ensure the best results in your trained models. User data is not used to train the recommender in this practice example.\n",
    "\n",
    "In this notebook, interactions and item data is imported into your environment. The data is inspected and converted to a format that can be used in Amazon Personalize to train models to get personalized recommendations.\n",
    "\n",
    "#### Cell 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the SDK to Amazon Personalize\n",
    "personalize = boto3.client('personalize')\n",
    "personalize_runtime = boto3.client('personalize-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad899459",
   "metadata": {},
   "source": [
    "### Creating Amazon Personalize resources and importing data \n",
    "\n",
    "#### Get the account ID and Region\n",
    "\n",
    "#### Cell 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2d6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "print(\"account id:\", account_id)\n",
    "\n",
    "with open('/opt/ml/metadata/resource-metadata.json') as notebook_info:\n",
    "    data = json.load(notebook_info)\n",
    "    resource_arn = data['ResourceArn']\n",
    "    region = resource_arn.split(':')[3]\n",
    "print(\"region:\", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8e4cee",
   "metadata": {},
   "source": [
    "### IAM role\n",
    "\n",
    "Amazon Personalize needs the ability to assume AWS Identity and Access Management (IAM) roles to have the permissions to run certain tasks and access an Amazon Simple Storage Service (Amazon S3) data bucket for the files needed.  \n",
    "\n",
    "#### Cell 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfa2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Boto3 client to access IAM functions \n",
    "iam = boto3.client('iam')\n",
    "\n",
    "#  A role has been set up for this solution. The following obtains the ARN for that role \n",
    "#  and also prints the role name for your information\n",
    "\n",
    "role_name = iam.get_role(RoleName='personalize_exec_role')\n",
    "role_arn = role_name['Role']['Arn']\n",
    "\n",
    "role_name = role_arn.split('/')[1]\n",
    "role_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c38db3",
   "metadata": {},
   "source": [
    "### S3 bucket\n",
    "\n",
    "So far, we have downloaded, manipulated, and saved the data on the Amazon Elastic Block Store (Amazon EBS) instance attached to instance running this Jupyter notebook. \n",
    "\n",
    "By default, Amazon Personalize does not have permission to access the data uploaded to the S3 bucket in this practice account. To grant access to the Amazon Personalize to read the practice CSVs, you must set a bucket policy and create an IAM role that Amazon Personalize will assume. \n",
    "\n",
    "Use the metadata stored on the instance underlying this notebook to determine the Region that it operates in. (If you were using a Jupyter notebook outside of Amazon SageMaker, you'd simply define the Region as the following string. The S3 bucket must be in the same Region as the Amazon Personalize resources that have been created so far.\n",
    "\n",
    "#### Cell 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed29ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Boto3 client to access S3 functions \n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Get a list of all S3 buckets so that we can find the one that starts with \"personalized-marketing\"\n",
    "response = s3.list_buckets()\n",
    "\n",
    "# Filter buckets that start with 'personalized-marketing'\n",
    "buckets_list = [bucket['Name'] for bucket in response['Buckets'] if bucket['Name'].startswith('personalized-marketing')]\n",
    "\n",
    "# Get the one bucket name from the list\n",
    "for data_bucket in buckets_list:\n",
    "    data_bucket_name = data_bucket\n",
    "\n",
    "# Display the name of the bucket found    \n",
    "data_bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9bddec",
   "metadata": {},
   "source": [
    "### Upload data to Amazon S3\n",
    "\n",
    "Now, upload the CSV files of our two datasets, items and interactions.\n",
    "\n",
    "#### Cell 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d934477",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_filename = 'interactions.csv'\n",
    "items_filename = \"items.csv\"\n",
    "\n",
    "interactions_file = interactions_filename\n",
    "\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=data_bucket_name,\n",
    "        Key=interactions_filename,\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(interactions_filename, data_bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    # Uploading the file if it does not already exist\n",
    "    boto3.Session().resource('s3').Bucket(data_bucket_name).Object(interactions_filename).upload_file(interactions_filename)\n",
    "    print(\"File {} uploaded to bucket {}\".format(interactions_filename, data_bucket_name))\n",
    "\n",
    "items_file = \"imdb/\" + items_filename\n",
    "\n",
    "try:\n",
    "    s3.get_object(\n",
    "        Bucket=data_bucket_name,\n",
    "        Key=items_filename,\n",
    "    )\n",
    "    print(\"{} already exists in the bucket {}\".format(items_file, data_bucket_name))\n",
    "except s3.exceptions.NoSuchKey:\n",
    "    # Uploading the file if it does not already exist\n",
    "    # Note that the following line will be needed for the DIY     \n",
    "    boto3.Session().resource('s3').Bucket(data_bucket_name).Object(items_filename).upload_file(items_file)\n",
    "    print(\"File {} uploaded to bucket {}\".format(items_filename, data_bucket_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1bf32",
   "metadata": {},
   "source": [
    "## Create the dataset group\n",
    "\n",
    "The highest level of isolation and abstraction with Amazon Personalize is a *dataset group*. Information stored within one of these dataset groups has no impact on any other dataset group or models created from one — they are completely isolated. This way you can run many experiments, which is part of how your models are kept private and fully trained on only your data. \n",
    "\n",
    "Before importing the data prepared earlier, a dataset group and a dataset must be added that handles the interactions.\n",
    "\n",
    "Dataset groups can house the following types of information:\n",
    "\n",
    "* User-item-interactions\n",
    "* Event streams (real-time interactions)\n",
    "* User metadata\n",
    "* Item metadata\n",
    "\n",
    "A dataset group must be created that contain your three datasets. Your dataset group can be one of the following types:\n",
    "\n",
    "* A Domain dataset group is where you create preconfigured resources for different business domains and use cases, such as getting recommendations for similar videos (VIDEO_ON_DEMAND domain) or best-selling items (ECOMMERCE domain). You choose your business domain, import your data, and create recommenders. You use recommenders in your application to get recommendations. Use a Domain dataset group if you have a video on demand or ecommerce application and want Amazon Personalize to find the best configurations for your use cases. If you start with a Domain dataset group, you can also add custom resources such as solutions with solution versions trained with recipes for custom use cases.\n",
    "\n",
    "* A Custom dataset group is where you create configurable resources for custom use cases and batch recommendation workflows. You choose a recipe, train a solution version (model), and deploy the solution version with a campaign. You use a campaign in your application to get recommendations. Use a Custom dataset group if you don't have a video on demand or ecommerce application or want to configure and manage only custom resources, or want to get recommendations in a batch workflow. If you start with a Custom dataset group, you can't associate it with a domain later. Instead, create a new Domain dataset group.\n",
    "\n",
    "You can create and manage Domain dataset groups and Custom dataset groups on the AWS Management console, the AWS Command Line Interface (AWS CLI), or programmatically with the AWS SDKs.\n",
    "\n",
    "In this solution, you create a Domain dataset group.\n",
    "\n",
    "#### Cell 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e619950",
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_dataset_group_name = \"marketing-email-dataset\"\n",
    "try:     \n",
    "    # Try to create the dataset group. This block will run fully if the dataset group does not exist yet\n",
    "    # Refer to this section for the DIY\n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = marketing_dataset_group_name,\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "\n",
    "    marketing_dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print(json.dumps(create_dataset_group_response, indent=2))\n",
    "    print ('\\nCreating the Dataset Group with dataset_group_arn = {}'.format(marketing_dataset_group_arn))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    # If the dataset group already exists, get the unique identifier, marketing_dataset_group_arn, \n",
    "    # from the existing resource\n",
    "    \n",
    "    marketing_dataset_group_arn = 'arn:aws:personalize:'+region+':'+account_id+':dataset-group/'+marketing_dataset_group_name \n",
    "    print ('\\nThe the Dataset Group with dataset_group_arn = {} already exists'.format(marketing_dataset_group_arn))\n",
    "    print ('\\nWe will be using the existing Dataset Group dataset_group_arn = {}'.format(marketing_dataset_group_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5420042",
   "metadata": {},
   "source": [
    "#### Wait for the dataset group to have an ACTIVE status. \n",
    "\n",
    "Before you can use the dataset group to create more resources, the dataset group must be active. This should take a minute or two. Run the next code cell, and then wait for it to show the ACTIVE status. It checks the status of the dataset group every 30 seconds.\n",
    "\n",
    "#### Cell 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77c5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60 # 3 minutes\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = marketing_dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1405534",
   "metadata": {},
   "source": [
    "## Create the interactions schema\n",
    "\n",
    "Now that you've loaded and prepared the three datasets, you'll configure Amazon Personalize to understand the data so that the service can be used to train models to generate recommendations. Amazon Personalize requires a schema for each dataset, so it can map the columns in the CSVs to fields for model training. Each schema is declared in JSON using the [Apache Avro](https://avro.apache.org/) format. \n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. Several mandatory fields are required in the schema, depending on the type of dataset. For more information, see Schemas in the Amazon Personalize Developer Guide at https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html.\n",
    "\n",
    "The interactions dataset has three required columns: `ITEM_ID`, `USER_ID`, and `TIMESTAMP`. The `TIMESTAMP` column represents when the user interacted with an item, and it must be expressed in Unix timestamp format (seconds). This dataset also has an `EVENT_TYPE` column. Columns must be defined in the same order in the schema as they appear in the dataset.\n",
    "\n",
    "#### Cell 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe36498",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_schema_name = \"marketing_interactions_schema\"\n",
    "\n",
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EVENT_TYPE\", # \"Watch\", \"Click\", etc.\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Try to create the interactions dataset schema. This block will run fully \n",
    "    # if the interactions dataset schema does not exist yet\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = interactions_schema_name,\n",
    "        schema = json.dumps(interactions_schema),\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    marketing_interactions_schema_arn = create_schema_response['schemaArn']\n",
    "    print ('\\nCreating the Interactions Schema with marketing_interactions_schema_arn = {}'.format(marketing_interactions_schema_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # If the interactions dataset schema already exists, get the unique identifier marketing_interactions_schema_arn\n",
    "    # from the existing resource \n",
    "    \n",
    "    marketing_interactions_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+interactions_schema_name \n",
    "    print('The schema {} already exists.'.format(marketing_interactions_schema_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Schema with marketing_interactions_schema_arn = {}'.format(marketing_interactions_schema_arn))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d95677",
   "metadata": {},
   "source": [
    "## Create the interactions dataset\n",
    "\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. \n",
    "\n",
    "#### Cell 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb6d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_dataset_name = \"marketing_interactions\"\n",
    "try:\n",
    "    # Try to create the interactions dataset. This block will run fully \n",
    "    # if the interactions dataset does not exist yet\n",
    "    \n",
    "    dataset_type = 'INTERACTIONS'\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = interactions_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = marketing_dataset_group_arn,\n",
    "        schemaArn = marketing_interactions_schema_arn\n",
    "    )\n",
    "\n",
    "    marketing_interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "    print ('\\nCreating the Interactions Dataset with marketing_interactions_dataset_arn = {}'.format(marketing_interactions_dataset_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # If the interactions dataset already exists, get the unique identifier, marketing_interactions_dataset_arn, \n",
    "    # from the existing resource \n",
    "    marketing_interactions_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+marketing_dataset_group_name+'/INTERACTIONS'\n",
    "    print('The Interactions Dataset {} already exists.'.format(marketing_interactions_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Dataset with marketing_interactions_dataset_arn = {}'.format(marketing_interactions_dataset_arn))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a796c53d",
   "metadata": {},
   "source": [
    "## Create the items (movies) schema\n",
    "\n",
    "First, define a schema to tell Amazon Personalize what type of dataset you are uploading. Several reserved and mandatory keywords are required in the schema, based on the type of dataset. For more information, see Schemas in the Amazon Personalize Developer Guide at https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html.\n",
    "\n",
    "The items metadata data has the following columns: `ITEM_ID`, `TITLE`, `YEAR`, `IMDB_RATING`,`IMDB_NUMBEROFVOTES`, `US_MATURITY_RATING_STRING`, `US_MATURITY_RATING`,`GENRES`, `CREATION_TIMESTAMP`, and `PROMOTION`. These columns must be defined in the same order in the schema as they appear in the dataset.\n",
    "\n",
    "#### Cell 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9459d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_schema_name = \"marketing_items_schema\"\n",
    "\n",
    "items_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Items\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TITLE\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"YEAR\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"IMDB_RATING\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"IMDB_NUMBEROFVOTES\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"US_MATURITY_RATING_STRING\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"US_MATURITY_RATING\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENRES\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CREATION_TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PROMOTION\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Try to create the items dataset schema. This block will run fully \n",
    "    # if the items dataset schema does not exist yet\n",
    "    \n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = items_schema_name,\n",
    "        schema = json.dumps(items_schema),\n",
    "        domain='VIDEO_ON_DEMAND'\n",
    "    )\n",
    "    marketing_items_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Items Schema with marketing_items_schema_arn = {}'.format(marketing_items_schema_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # If the items dataset schema already exists, get the unique identifier, marketing_items_schema_arn, \n",
    "    # from the existing resource \n",
    "    \n",
    "    marketing_items_schema_arn = 'arn:aws:personalize:'+region+':'+account_id+':schema/'+items_schema_name \n",
    "    print('The schema {} already exists.'.format(marketing_items_schema_arn))\n",
    "    print ('\\nWe will be using the existing Items Schema with marketing_items_schema_arn = {}'.format(marketing_items_schema_arn))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368bcf5f",
   "metadata": {},
   "source": [
    "## Create the items dataset\n",
    "\n",
    "With a schema created, you can create a dataset within the dataset group. Note that this does not load the data yet, but creates a schema of what the data looks like. \n",
    "\n",
    "#### Cell 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_dataset_name = \"marketing_items\"\n",
    "\n",
    "try:\n",
    "    # Try to create the items dataset. This block will run fully if the items dataset does not exist yet\n",
    "    dataset_type = \"ITEMS\"\n",
    "    # Refer to the following code for the DIY\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = items_dataset_name,\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = marketing_dataset_group_arn,\n",
    "        schemaArn = marketing_items_schema_arn\n",
    "    )\n",
    "\n",
    "    marketing_items_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Items Dataset with marketing_items_dataset_arn = {}'.format(marketing_items_dataset_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # If the items dataset already exists, get the unique identifier, marketing_items_dataset_arn,\n",
    "    # from the existing resource \n",
    "    \n",
    "    marketing_items_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+marketing_dataset_group_name+'/ITEMS'\n",
    "    print('The Items Dataset {} already exists.'.format(marketing_items_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Items Dataset with marketing_items_dataset_arn = {}'.format(marketing_items_dataset_arn))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c48c4",
   "metadata": {},
   "source": [
    "Now, wait for all the datasets to be created.\n",
    "\n",
    "#### Cell 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6888569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start_time = time.time()\n",
    "max_time = time.time() + 6*60 # 6 Minutes\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = marketing_interactions_dataset_arn\n",
    "    )\n",
    "    status_interaction_dataset =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Interactions Dataset: {}\".format(status_interaction_dataset))\n",
    "    \n",
    "    if status_interaction_dataset == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(marketing_interactions_dataset_arn))\n",
    "        \n",
    "    elif status_interaction_dataset == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(marketing_interactions_dataset_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_interaction_dataset == \"ACTIVE\":\n",
    "        print(\"The interaction dataset creation is still in progress\")\n",
    "    else:\n",
    "        print(\"The interaction dataset  is ACTIVE\")\n",
    "        \n",
    "\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = marketing_items_dataset_arn\n",
    "    )\n",
    "    status_item_dataset =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Items Dataset: {}\".format(status_item_dataset))\n",
    "    \n",
    "    if status_item_dataset == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(marketing_items_dataset_arn))\n",
    "        \n",
    "    elif status_item_dataset == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(marketing_items_dataset_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_item_dataset == \"ACTIVE\":\n",
    "        print(\"The item dataset creation is still in progress\")\n",
    "    else:\n",
    "        print(\"The item dataset  is ACTIVE\")\n",
    "        \n",
    "    if status_interaction_dataset == \"ACTIVE\" and status_item_dataset == \"ACTIVE\":\n",
    "        end_time = time.time()\n",
    "        break\n",
    "    time.sleep(15)\n",
    "\n",
    "time_elapsed = end_time - start_time\n",
    "print(f\"Time elapsed: {time_elapsed} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baa529b",
   "metadata": {},
   "source": [
    "## Import the interactions data \n",
    "\n",
    "Earlier, the dataset group and dataset were created to house the information. Now, an import job is run to load the interactions data from the S3 bucket into the Amazon Personalize dataset. \n",
    "\n",
    "#### Cell 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb03dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_import_job_name = \"dataset_import_interaction\"\n",
    "# Check if the import job already exists\n",
    "\n",
    "# List the import jobs\n",
    "interactions_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=marketing_interactions_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "# Check if there is an existing job with the prefix\n",
    "job_exists = False  \n",
    "job_arn = None\n",
    "\n",
    "for job in interactions_dataset_import_jobs:\n",
    "    if (interactions_import_job_name in job['jobName']):\n",
    "        job_exists = True\n",
    "        job_arn = job['datasetImportJobArn']\n",
    "    \n",
    "if (job_exists):\n",
    "    marketing_interactions_dataset_import_job_arn = job_arn\n",
    "    print('The Interactions Import Job {} already exists.'.format(marketing_interactions_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Interactions Import Job with marketing_interactions_dataset_import_job_arn = {}'.format(marketing_interactions_dataset_import_job_arn))\n",
    "        \n",
    "else:\n",
    "    # If there is no import job with the prefix, create it   \n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = interactions_import_job_name,\n",
    "        datasetArn = marketing_interactions_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": f\"s3://{data_bucket_name}/interactions.csv\"\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "    marketing_interactions_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    \n",
    "    print ('\\nImporting the Interactions Data with marketing_interactions_dataset_import_job_arn = {}'.format(marketing_interactions_dataset_import_job_arn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da801e7f",
   "metadata": {},
   "source": [
    "## Import the item metadata \n",
    "\n",
    "#### Cell 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da2c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_import_job_name = \"dataset_import_item\"\n",
    "\n",
    "# Check if the import job already exists\n",
    "\n",
    "# List the import jobs\n",
    "items_dataset_import_jobs = personalize.list_dataset_import_jobs(\n",
    "    datasetArn=marketing_items_dataset_arn,\n",
    "    maxResults=100\n",
    ")['datasetImportJobs']\n",
    "\n",
    "job_exists = False\n",
    "job_arn = None\n",
    "\n",
    "print (items_dataset_import_jobs)\n",
    "\n",
    "# Check if there is an existing job with the prefix\n",
    "for job in items_dataset_import_jobs:\n",
    "    if (items_import_job_name in job['jobName']):\n",
    "        job_exists = True\n",
    "        job_arn = job['datasetImportJobArn']\n",
    "    \n",
    "if (job_exists):\n",
    "    marketing_items_dataset_import_job_arn =  job_arn\n",
    "    print('The Items Import Job {} already exists.'.format(marketing_items_dataset_import_job_arn))\n",
    "    print ('\\nWe will be using the existing Items Import Job with marketing_items_dataset_import_job_arn = {}'.format(marketing_items_dataset_import_job_arn))\n",
    "        \n",
    "else:\n",
    "    # If there is no import job with the prefix, create it    \n",
    "    # Refer to the following code for the DIY\n",
    "    create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "        jobName = items_import_job_name,\n",
    "        datasetArn = marketing_items_dataset_arn,\n",
    "        dataSource = {\n",
    "            \"dataLocation\": f\"s3://{data_bucket_name}/items.csv\"\n",
    "        },\n",
    "        roleArn = role_arn\n",
    "    )\n",
    "\n",
    "    marketing_items_dataset_import_job_arn = create_dataset_import_job_response['datasetImportJobArn']\n",
    "    print(json.dumps(create_dataset_import_job_response, indent=2))\n",
    "    print ('\\nImporting the Items Data with marketing_items_dataset_import_job_arn = {}'.format(marketing_items_dataset_import_job_arn))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feddda89",
   "metadata": {},
   "source": [
    "Before you can use the dataset, the import job must be active. Run the next code cell, and then wait for it to show the ACTIVE status. It checks the status of the import job every minute.\n",
    "\n",
    "Importing the data takes about 15 minutes. \n",
    "\n",
    "You must wait for the data imports to be completed.\n",
    "\n",
    "#### Cell 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7989a711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_time = time.time() + 20*60 # 20 minutes\n",
    "start_time = time.time()\n",
    "while time.time() < max_time:\n",
    "\n",
    "    # Interactions dataset import\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = marketing_interactions_dataset_import_job_arn\n",
    "    )\n",
    "    status_interactions_import = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    \n",
    "    if status_interactions_import == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(marketing_interactions_dataset_import_job_arn))\n",
    "        \n",
    "    elif status_interactions_import == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(marketing_interactions_dataset_import_job_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_interactions_import == \"ACTIVE\":\n",
    "        print(\"The interactions dataset import is still in progress\")\n",
    "    else:\n",
    "        print(\"The interactions dataset import is ACTIVE\")\n",
    "\n",
    "    # Items dataset import\n",
    "    describe_dataset_import_job_response = personalize.describe_dataset_import_job(\n",
    "        datasetImportJobArn = marketing_items_dataset_import_job_arn\n",
    "    )\n",
    "    status_items_import = describe_dataset_import_job_response[\"datasetImportJob\"]['status']\n",
    "    \n",
    "    if status_items_import == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(marketing_items_dataset_import_job_arn))\n",
    "        \n",
    "    elif status_items_import == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(marketing_items_dataset_import_job_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_items_import == \"ACTIVE\":\n",
    "        print(\"The items dataset import is still in progress\")\n",
    "    else:\n",
    "        print(\"The items dataset import is ACTIVE\")\n",
    "\n",
    "    if status_interactions_import == \"ACTIVE\" and status_items_import == 'ACTIVE':\n",
    "        end_time = time.time()\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    time.sleep(30)\n",
    "    \n",
    "time_elapsed = end_time - start_time\n",
    "print(f\"Time elapsed: {time_elapsed} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5993e9c1",
   "metadata": {},
   "source": [
    "## Create a \"Top picks for you\" recommender\n",
    "\n",
    "Create a preconfigured VIDEO_ON_DEMAND recommender that matches the use case. \n",
    "\n",
    "Each domain has different use cases. When you create a recommender, you create it for a specific use case, and each use case has different requirements for getting recommendations.\n",
    "\n",
    "Explore the recommenders supported for the VIDEO_ON_DEMAND domain.\n",
    "\n",
    "#### Cell 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_recipes = personalize.list_recipes(domain='VIDEO_ON_DEMAND')\n",
    "display_available_recipes = available_recipes ['recipes']\n",
    "available_recipes = personalize.list_recipes(domain='VIDEO_ON_DEMAND',nextToken=available_recipes['nextToken'])#paging to get the rest of the recipes \n",
    "display_available_recipes = display_available_recipes + available_recipes['recipes']\n",
    "display(display_available_recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4621c3",
   "metadata": {},
   "source": [
    "Create a \"Top picks for you\" recommender. This type of recommender offers personalized streaming content recommendations for a user that you specify. With this use case, Amazon Personalize automatically filters videos that the user watched, based on the user ID (that you specify) and watch events.\n",
    "\n",
    "#### Cell 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender_top_picks_for_you_name = \"marketing_top_picks_for_you\"\n",
    "\n",
    "try:\n",
    "    create_recommender_response = personalize.create_recommender(\n",
    "        name = recommender_top_picks_for_you_name,\n",
    "        recipeArn = 'arn:aws:personalize:::recipe/aws-vod-top-picks',\n",
    "        datasetGroupArn = marketing_dataset_group_arn,\n",
    "        recommenderConfig = {\"enableMetadataWithRecommendations\": True}\n",
    "    )\n",
    "    marketing_recommender_top_picks_arn = create_recommender_response[\"recommenderArn\"]\n",
    "    \n",
    "    print (json.dumps(create_recommender_response))\n",
    "    print ('\\nCreating the Top Picks For You recommender with marketing_recommender_top_picks_arn = {}'.format(marketing_recommender_top_picks_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    marketing_recommender_top_picks_arn =  'arn:aws:personalize:'+region+':'+account_id+':recommender/'+recommender_top_picks_for_you_name\n",
    "    print('The Top Picks For You recommender {} already exists.'.format(marketing_recommender_top_picks_arn))\n",
    "    print ('\\nWe will be using the existing Top Picks For You recommender with marketing_recommender_top_picks_arn = {}'.format(marketing_recommender_top_picks_arn))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaf5bc2",
   "metadata": {},
   "source": [
    "### View the recommender creation status\n",
    "\n",
    "Set up a loop to see the creation status of the recommender. This can take more than 60 minutes to train. \n",
    "\n",
    "#### Cell 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5214ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 10*60*60 # 10 hours\n",
    "start_time = time.time()\n",
    "while time.time() < max_time:\n",
    "\n",
    "    # Recommender top_picks_for_you\n",
    "    version_response = personalize.describe_recommender(\n",
    "        recommenderArn = marketing_recommender_top_picks_arn\n",
    "    )\n",
    "    status_top_picks = version_response[\"recommender\"][\"status\"]\n",
    "\n",
    "    if status_top_picks == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(marketing_recommender_top_picks_arn))\n",
    "    elif status_top_picks == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(marketing_recommender_top_picks_arn))\n",
    "        break\n",
    "\n",
    "    if not status_top_picks == \"ACTIVE\":\n",
    "        print(\"The Top Picks for Your recommender build is still in progress\")\n",
    "    else:\n",
    "        print(\"The Top Picks for Your recommender is ACTIVE\")\n",
    "\n",
    "    if status_top_picks == 'ACTIVE':\n",
    "        end_time = time.time()\n",
    "        break\n",
    "    print()\n",
    "    time.sleep(60)\n",
    "    \n",
    "time_elapsed = end_time - start_time\n",
    "print(f\"Time elapsed: {time_elapsed} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9821ac5",
   "metadata": {},
   "source": [
    "## Get personalized recommendations from Amazon Personalize\n",
    "\n",
    "Select a random user to see their recommendations.\n",
    "\n",
    "#### Cell 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd28109",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = random.sample(list(user_ids), 1)[0]\n",
    "user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b3340b",
   "metadata": {},
   "source": [
    "Get 15 recommendations from the \"Top pics for you\" recommender that you trained.\n",
    "\n",
    "#### Cell 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed7ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "    recommenderArn = marketing_recommender_top_picks_arn,\n",
    "    userId = str(user_id),\n",
    "    numResults = 15,\n",
    "    metadataColumns = {\n",
    "        \"ITEMS\": ['TITLE', 'GENRES']\n",
    "    }\n",
    ")\n",
    "\n",
    "print (get_recommendations_response['itemList'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca5d1cb",
   "metadata": {},
   "source": [
    "Getting recomendations works!\n",
    "\n",
    "To get recommended movies and their metadata for each user, a more user-friendly access method can be created.\n",
    "\n",
    "#### Cell 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecommendedMoviesForUserId(\n",
    "    user_id, \n",
    "    marketing_recommender_top_picks_arn, \n",
    "    item_data, \n",
    "    number_of_movies_to_recommend = 5):\n",
    "    # For a user_id, get the top n (number_of_movies_to_recommend) movies by using Amazon Personalize \n",
    "    # and get the additional metadata for each movie (item_id) from the item_data\n",
    "    # Return a list of movie dictionaries (movie_list) with the relevant data\n",
    "\n",
    "    # Get recommended movies\n",
    "    get_recommendations_response = personalize_runtime.get_recommendations(\n",
    "        recommenderArn = marketing_recommender_top_picks_arn,\n",
    "        userId = str(user_id),\n",
    "        numResults = number_of_movies_to_recommend,\n",
    "        metadataColumns = {\n",
    "            \"ITEMS\": ['TITLE', 'GENRES']\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create a list of movies with title, genres \n",
    "    movie_list = []\n",
    "    \n",
    "    for recommended_movie in get_recommendations_response['itemList']:      \n",
    "        movie_list.append(\n",
    "            {\n",
    "                'title' : recommended_movie['metadata']['title'],\n",
    "                'genres' : recommended_movie['metadata']['genres'].replace('|', ' and ')\n",
    "            }\n",
    "        )\n",
    "    return movie_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95bf8f5",
   "metadata": {},
   "source": [
    "A random user is selected next, and three movies are recommended for that user.  \n",
    "\n",
    "Note that because users change each time, recommendations are different each time this code cell runs.\n",
    "\n",
    "#### Cell 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdbc807",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = random.sample(list(user_ids), 1)[0]\n",
    "number_of_movies_to_recommend = 3 \n",
    "\n",
    "movie_list = getRecommendedMoviesForUserId(user_id, marketing_recommender_top_picks_arn, item_data, number_of_movies_to_recommend)\n",
    "\n",
    "# Print each movie in the array\n",
    "for movie in movie_list:\n",
    "    print ('Title: '+movie['title'])\n",
    "    print ('Genres: '+movie['genres'])\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a92b00b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5be55daa",
   "metadata": {},
   "source": [
    "#### Cell 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014752b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join([f\"{movie['title']} ({movie['genres']})\" for movie in movie_list])\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2ee85",
   "metadata": {},
   "source": [
    "## Get the user's favorite movie genre\n",
    "\n",
    "To provide a better personalized marketing communication, in this section, you calculate a user's favorite movie genre based on the genres of all the movies they interacted with in the past.\n",
    "\n",
    "#### Cell 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b06b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserFavouriteGenres(user_id, interactions_df, movie_data):\n",
    "    # For a user_id, get the user's favorite genre by looking at the user's interactions \n",
    "    # with each movie in the past and counting the genres to find the most common genre \n",
    "\n",
    "    # Get all movies the user has watched     \n",
    "    movies_df = interactions_df[interactions_df['USER_ID'] == user_id]\n",
    "\n",
    "    genres = {}\n",
    "\n",
    "    for movie_id in movies_df['ITEM_ID']:\n",
    "\n",
    "        movie_genres = movie_data[movie_data['ITEM_ID']==movie_id]['GENRES']\n",
    "        \n",
    "        if not len(movie_genres.tolist())==0:\n",
    "            for movie_genre in movie_genres.tolist()[0].split('|'):\n",
    "                if movie_genre in genres:\n",
    "                    genres[movie_genre] +=1\n",
    "                else:\n",
    "                    genres[movie_genre] = 1\n",
    "\n",
    "    genres_df = pd.DataFrame(list(genres.items()), columns =['GENRE', 'COUNT'])\n",
    "    \n",
    "    # Sort by most common\n",
    "    genres_df.sort_values(by=['COUNT'], inplace=True, ascending = False)\n",
    "    \n",
    "    # Return the most common (favorite) genre       \n",
    "    return genres_df.iloc[[0]]['GENRE'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67033aa1",
   "metadata": {},
   "source": [
    "#### Cell 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_favorite_genre = getUserFavouriteGenres(user_id, interactions_df, item_data)\n",
    "user_favorite_genre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d2458d",
   "metadata": {},
   "source": [
    "## Using Amazon Bedrock\n",
    "\n",
    "Now that you have personalized recommendations for each user, communications can be sent out.  Rather than sending out a generic form email to each user, Amazon Bedrock can help you create custom emails for each user.\n",
    "\n",
    "#### Cell 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de1865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Boto3 client to access the functions within Amazon Bedrock\n",
    "bedrock = boto3.client('bedrock-runtime') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941df6d",
   "metadata": {},
   "source": [
    "Set up some parameters needed to access the slected Amazon Bedrock model. In this practice example, you use the Amazon Titan Text G1 - Lite foundation model (FM).\n",
    "\n",
    "Note: At the time of this writing, FM access is not turned on by default. The practice example will fail if you did not follow the earlier step, which requested access to this Amazon Titan model.\n",
    "\n",
    "#### Cell 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed9f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "# The LLM you will be using\n",
    "model_id = 'amazon.titan-text-lite-v1'\n",
    "\n",
    "# The desired MIME type of the inference body in the response\n",
    "accept = 'application/json'\n",
    "\n",
    "# The MIME type of the input data in the request\n",
    "content_type = 'application/json'\n",
    "\n",
    "# The maximum number of tokens to use in the generated response\n",
    "max_tokens_to_sample = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4dde17",
   "metadata": {},
   "source": [
    "## Add user demographic information\n",
    "\n",
    "Generate emails by assuming two different demographics for the users.\n",
    "\n",
    "#### Cell 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample user demographics\n",
    "user_demographic_1 = f'The user is a 50 year old adult called Otto.'\n",
    "user_demographic_3 = f'The user is a young adult called Jane.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878b6b12",
   "metadata": {},
   "source": [
    "## Generate personalized marketing emails\n",
    "\n",
    "Generating a marketing email requires a prompt that tells the FM what you want it to do.\n",
    "\n",
    "#### Cell 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_personalized_prompt(user_demographic, favorite_genre, movie_list, model_id, max_tokens_to_sample = 50):\n",
    "\n",
    "    prompt_template = f'''You are a skilled publicist. Write a high-converting marketing email advertising several movies available in a video-on-demand streaming platform next week, \n",
    "    given the movie and user information below. Your email will leverage the power of storytelling and persuasive language. \n",
    "    You want the email to impress the user, so make it appealing to them based on the information contained in the <user> tags, \n",
    "    and take into account the user's favorite genre in the <genre> tags. \n",
    "    The movies to recommend and their information is contained in the <movie> tag. \n",
    "    All movies in the <movie> tag must be recommended. Give a summary of the movies and why the human should watch them. \n",
    "    Put the email between <email> tags.\n",
    "    Sign it from \"Cloud island movies\".\n",
    "    \n",
    "    <user>\n",
    "    {user_demographic}\n",
    "    </user>\n",
    "\n",
    "    <genre>\n",
    "    {favorite_genre}\n",
    "    </genre>\n",
    "\n",
    "    <movie>\n",
    "    {movie_list}\n",
    "    </movie>\n",
    "\n",
    "    '''\n",
    "\n",
    "    prompt_input = json.dumps({\n",
    "        \"inputText\":prompt_template,\n",
    "        \"textGenerationConfig\": {\n",
    "            \"maxTokenCount\": 4096,\n",
    "            \"stopSequences\": [],\n",
    "            \"temperature\": 0.7,\n",
    "            \"topP\": 0.9\n",
    "        }\n",
    "    })\n",
    "      \n",
    "    return prompt_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94eeb5c",
   "metadata": {},
   "source": [
    "Start with demographic 1, which is a 50-year-old user.\n",
    "\n",
    "#### Cell 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55130628",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('User\\'s demographic')\n",
    "user_demographic = user_demographic_1\n",
    "user_demographic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c545dc",
   "metadata": {},
   "source": [
    "Next, use the prompt info from the previous code cell to create a prompt input that is JSON formatted.\n",
    "\n",
    "#### Cell 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53124ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt input\n",
    "prompt_input_json = generate_personalized_prompt(user_demographic, user_favorite_genre, movie_list, model_id, max_tokens_to_sample )\n",
    "prompt_input_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c89f5c",
   "metadata": {},
   "source": [
    "Now, put it all together and generate that personalized marketing email!\n",
    "\n",
    "#### Cell 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a0e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock.invoke_model(\n",
    "    body= prompt_input_json,\n",
    "    modelId=model_id,\n",
    "    accept=accept,    \n",
    "    contentType=content_type\n",
    "    )\n",
    "\n",
    "response_body = json.loads(response.get('body').read())\n",
    "model_output_string = response_body['results'][0]['outputText']\n",
    "# model_output_str_clean = re.sub(r'<[^>]*>', '', model_output_string)\n",
    "\n",
    "print(model_output_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c0161f",
   "metadata": {},
   "source": [
    "# ------------ STOP --------------\n",
    "\n",
    "## Use the following cells for the DIY section of the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b672c",
   "metadata": {},
   "source": [
    "The first step is to create a DataFrame that has only the movies that are rated G.\n",
    "\n",
    "#### Cell DIY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8658a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data_rated_g = item_data[item_data['US_MATURITY_RATING_STRING'] == 'G']\n",
    "\n",
    "item_data_rated_g.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea29ab7",
   "metadata": {},
   "source": [
    "Write the DataFrame out to a CSV file that can be uploaded to Amazon S3. Remember that Amazon Personalize must read the data from Amazon S3 to create the datasets.\n",
    "\n",
    "#### Cell DIY 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5387b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_data_rated_g.to_csv('items_rated_g.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2cbee",
   "metadata": {},
   "source": [
    "The file created in the previous code cell must now be written to Amazon S3.  Refer to the earlier Cell 10 for the needed Boto3 command, and ensure that the new variable names are substituted for what was used before.\n",
    "\n",
    "#### Cell DIY 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_rated_g_filename = \"items_rated_g.csv\"\n",
    "items_rated_g_file = items_rated_g_filename\n",
    "\n",
    "'''\n",
    "For code copied from Cell 10, ensure that item_filename and \n",
    "item_file are both replaced with the new variable names above.\n",
    "\n",
    "Also be aware of code indention to avoid errors. \n",
    "'''\n",
    "\n",
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becee84d",
   "metadata": {},
   "source": [
    "Now that the file is in Amazon S3, create the new dataset group and dataset within Amazon Personalize. Start by creating the dataset group.  Refer to the earlier Cell 11 as needed, and ensure that the new group name is used.\n",
    "\n",
    "#### Cell DIY 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6db56cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_dataset_rated_g_group_name = \"marketing_email_dataset_rated_g\"\n",
    "'''\n",
    "For code copied from Cell 11, ensure that marketing_dataset_group_name \n",
    "is replaced with the new variable name above.\n",
    "\n",
    "Also be aware of code indention to avoid errors. \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:     \n",
    "    # Put your code here\n",
    "\n",
    "    marketing_dataset_rated_g_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print(json.dumps(create_dataset_group_response, indent=2))\n",
    "    print ('\\nCreating the Dataset Group with dataset_group_arn = {}'.format(marketing_dataset_rated_g_group_arn))\n",
    "\n",
    "except personalize.exceptions.ResourceAlreadyExistsException as e:\n",
    "    # If the dataset group already exists, get the unique identifier for marketing_dataset_rated_g_group_arn \n",
    "    # from the existing resource\n",
    "    \n",
    "    marketing_dataset_rated_g_group_arn = 'arn:aws:personalize:'+region+':'+account_id+':dataset-group/'+marketing_dataset_rated_g_group_name \n",
    "    print ('\\nThe the Dataset Group with dataset_group_arn = {} already exists'.format(marketing_dataset_rated_g_group_arn))\n",
    "    print ('\\nWe will be using the existing Dataset Group marketing_dataset_rated_g_group_arn = {}'.format(marketing_dataset_rated_g_group_arn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72addfe",
   "metadata": {},
   "source": [
    "Be sure that the dataset group is active.\n",
    "\n",
    "#### Cell DIY 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = time.time() + 3*60 # 3 minutes\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = marketing_dataset_rated_g_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d08cc",
   "metadata": {},
   "source": [
    "Now, create the items dataset within the dataset group.  Refer to the earlier Cell 16 as needed.  Note that because the schema is the same, it can be reused rather than recreated.\n",
    "\n",
    "#### Cell DIY 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae2ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_rated_g_dataset_name = \"marketing_items_rated_g\"\n",
    "'''\n",
    "For code copied from Cell 16, ensure that items_dataset_name \n",
    "is replaced with the new variable name above. Also ensure to replace\n",
    "marketing_dataset_group_arn with marketing_dataset_rated_g_group_arn \n",
    "as used in DIY cells 4 & 5.\n",
    "\n",
    "Also be aware of code indention to avoid errors. \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    dataset_type = \"ITEMS\"\n",
    "    # Enter your code here\n",
    "    \n",
    "\n",
    "    marketing_items_rated_g_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "\n",
    "    print ('\\nCreating the Items Dataset with marketing_items_rated_g_dataset_arn = {}'.format(marketing_items_rated_g_dataset_arn))\n",
    "    \n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    # If the items dataset already exists, get the unique identifier, marketing_items_dataset_arn, \n",
    "    # from the existing resource \n",
    "    \n",
    "    marketing_items_rated_g_dataset_arn =  'arn:aws:personalize:'+region+':'+account_id+':dataset/'+marketing_dataset_rated_g_group_name+'/ITEMS'\n",
    "    print('The Items Dataset {} already exists.'.format(marketing_items_rated_g_dataset_arn))\n",
    "    print ('\\nWe will be using the existing Items Dataset with marketing_items_rated_g_dataset_arn = {}'.format(marketing_items_rated_g_dataset_arn))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23866e9",
   "metadata": {},
   "source": [
    "Be sure that the dataset is fully created.\n",
    "\n",
    "#### Cell DIY 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start_time = time.time()\n",
    "max_time = time.time() + 6*60 # 6 Minutes\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_response = personalize.describe_dataset(\n",
    "        datasetArn = marketing_items_rated_g_dataset_arn\n",
    "    )\n",
    "    status_item_dataset =  describe_dataset_response[\"dataset\"]['status']\n",
    "    print(\"Items Dataset: {}\".format(status_item_dataset))\n",
    "    \n",
    "    if status_item_dataset == \"ACTIVE\":\n",
    "        print(\"Build succeeded for {}\".format(marketing_items_rated_g_dataset_arn))\n",
    "        \n",
    "    elif status_item_dataset == \"CREATE FAILED\":\n",
    "        print(\"Build failed for {}\".format(marketing_items_rated_g_dataset_arn))\n",
    "        break\n",
    "        \n",
    "    if not status_item_dataset == \"ACTIVE\":\n",
    "        print(\"The item dataset creation is still in progress\")\n",
    "    else:\n",
    "        print(\"The item dataset  is ACTIVE\")\n",
    "        \n",
    "    if status_item_dataset == \"ACTIVE\":\n",
    "        end_time = time.time()\n",
    "        break\n",
    "    time.sleep(15)\n",
    "\n",
    "time_elapsed = end_time - start_time\n",
    "print(f\"Time elapsed: {time_elapsed} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87dc259",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfd51d",
   "metadata": {},
   "source": [
    "We will use the semantic search to provide the best matching book based on the author, genre and plot summary. [Retrieval Augmented Generation](https://arxiv.org/abs/2005.11401) is a process that combines retrieval-based models and generative models to enhance natural language generation by retrieving relevant information and incorporating it into the generation process. In this notebook, we'll walk through enhancing an OpenSearch cluster search with generative AI to output conversationa book recommendations based on a desired author, genre and plot summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c4d1e",
   "metadata": {},
   "source": [
    "### 1. Install OpenSearch ML Python library\n",
    "\n",
    "For this notebook we require the use of a few key libraries. We'll use the Python clients for OpenSearch and SageMaker, and Python frameworks for text embeddings.\n",
    "\n",
    "Note: Running of the code cell below does not produce any output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd6227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "## Code Cell 1 ##\n",
    "!pip install opensearch-py-ml accelerate tqdm --quiet\n",
    "!pip install sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31703e3d",
   "metadata": {},
   "source": [
    "### 2. Check PyTorch Version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac12126",
   "metadata": {},
   "source": [
    "let's import PyTorch and confirm that the latest version of PyTorch is running. The version should already be at 2.0.0 or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b532987",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 2 ##\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa614bc",
   "metadata": {},
   "source": [
    "### 3. Import libraries\n",
    "The line below will import all the relevant libraries and modules used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1688f4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 3 ##\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from sagemaker import get_execution_role\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6607721",
   "metadata": {},
   "source": [
    "### 4. Prepare data\n",
    "\n",
    "This lab combines semantic search with a generative model to present the retrieved data to the user . Below is a dataset of book information and plot summaries, we'll sample this data set to recommend books that resemble the user provided description.\n",
    "\n",
    "### Note\n",
    "The original data set is described here: \n",
    "https://www.cs.cmu.edu/~dbamman/booksummaries.html\n",
    "\n",
    "Run the following cells to inspect the dataset, transform it into a pandas DataFrame, clean-up missing values, and sample a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a462a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 4 ##\n",
    "\n",
    "# Read in the Tab delimited data file and print the shape of the resulting data frame.\n",
    "pd.options.mode.chained_assignment = None\n",
    "df = pd.read_csv('booksummaries.txt',sep='\\t')\n",
    "print(df.shape)\n",
    "\n",
    "# Add columns headers to the data frame to be able to analyze.\n",
    "df.columns = ['WikiPediaId','FreeBaseId','title','author','pub_date','genres','plot_summary']\n",
    "\n",
    "# Display entries with null data in any column (NaN).\n",
    "df[df.isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d17c2a-df78-46d4-bcab-8cc8f7a9fd71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 5 ##\n",
    "\n",
    "# Let's drop any rows that contain null values in any column. In a real production application you would want to replace NaN values with correct data.\n",
    "df_1 = df.dropna()\n",
    "\n",
    "# clean-up Freebase markup and other unwanted characters in the Genres columm. \n",
    "df_1.genres.replace(to_replace='\\\"\\/m\\/.{4,7}\\\"\\:', value='', regex=True,inplace=True)  # remove Freebase markup\n",
    "df_1.genres.replace(to_replace='\\\\\\\\u00e0\\sclef', value='', regex=True,inplace=True)    # remove utf-8 special characters\n",
    "df_1.genres.replace(to_replace='\\{\\s\"|\"\\}', value='', regex=True,inplace=True)          # Remove {\" or \"}\n",
    "df_1.genres.replace(to_replace='\"', value='', regex=True,inplace=True)                  # Remove double quotes\n",
    "\n",
    "# Only use the first value as the genre\n",
    "df_2 = df_1['genres'].str.split(',', expand=True, n=1)\n",
    "df_2.rename( columns={0:'genre'}, inplace=True )\n",
    "df_2.drop(columns=[1],inplace=True)\n",
    "\n",
    "\n",
    "df_3 = pd.concat([df_1, df_2], axis=1)\n",
    "\n",
    "# Trim the size of the plot summary to 500 characters.\n",
    "df_3['plot_summary'] = df_3['plot_summary'].apply(lambda x: ' '.join(x[:500].split(' ')[:-1]) if len(x) > 500 else x)\n",
    "df_3['book_summary'] = df_3[['title','author','genre','plot_summary']].agg(' '.join, axis=1)\n",
    "\n",
    "# Sort by the author column, and to keep the lab within a reasonable time-frame drop the last 5000 rows of data.\n",
    "# Drop other columns not needed.\n",
    "df_3.sort_values(by=['author'],inplace = True)\n",
    "df_3.drop(df_3.tail(5000).index,inplace = True)\n",
    "df_3.drop(columns=['genres','WikiPediaId','FreeBaseId','plot_summary'],inplace=True)\n",
    "\n",
    "# Create a dictionary of the remaining data to be used for further processing.\n",
    "wm_list = df_3.to_dict('records')\n",
    "\n",
    "# Let's look at the data now that it has been cleansed and trimmed.\n",
    "df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54a349",
   "metadata": {},
   "source": [
    "### 5. Create an OpenSearch cluster connection.\n",
    "Next, we'll use Python API to set up connection with OpenSearch Cluster.\n",
    "\n",
    "Note: if you're using a region other than us-east-1, please update the region in the code below. Also, be sure to replace the \\<StackName\\> with the value you copied \n",
    "    in an ealier step.\n",
    "\n",
    "You can ignore any \"PythonDeprecationWarning\" warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc45a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 6 ##\n",
    "\n",
    "## Update the below <StackName> placeholder with the value from your Lab you copied in an earlier step. \n",
    "cloudformation_stack_name = '<StackName>'\n",
    "\n",
    "region = 'us-east-1' \n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "aos_host = outputs['OSDomainEndpoint']\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e0e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 7 ##\n",
    "\n",
    "## To authenticate to the OpenSearch domain we need to retrieve username and password stored in Secrets Manager.\n",
    "secrets = boto3.client('secretsmanager')\n",
    "os_domain_secret = secrets.list_secrets(\n",
    "    Filters=[\n",
    "        {\n",
    "            'Key':'name',\n",
    "            'Values': ['DomainMasterUser']\n",
    "        }\n",
    "    ]\n",
    ")['SecretList'][0]['Name']\n",
    "\n",
    "aos_credentials = json.loads(secrets.get_secret_value(SecretId=os_domain_secret)['SecretString'])\n",
    "\n",
    "auth = (aos_credentials['username'], aos_credentials['password'])\n",
    "print(auth)\n",
    "\n",
    "## The below client will be used in a later step below.\n",
    "aos_client = OpenSearch(\n",
    "    hosts = [{'host': aos_host, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c1155",
   "metadata": {},
   "source": [
    "### 6. Get SageMaker endpoint for embedding\n",
    "\n",
    "---\n",
    "This is SageMaker Endpoint with Bloom 7B1 Embedding FP16 parameters model to convert text into vector.\n",
    "This transformer-based model from Hugging Face without a text generation model head, takes a text string as input and produces an embedding vector with 4096 dimensions. \n",
    "\n",
    "Note: Make sure to replace the \\<SageMaker_Embedding_Endpoint_Name\\> placeholder with the value you copied in an ealier step.\n",
    "\n",
    "Define function to convert text into vector with SageMaker Embedding endpoint created in an ealier step. Running of the code cell below does not produce any output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd689c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 8 ##\n",
    "\n",
    "embedding_endpoint_name = '<SageMaker_Embedding_Endpoint_Name>'\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name, content_type=\"application/json\"):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "   \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json\n",
    "    )\n",
    "\n",
    "    response_json = json.loads(response['Body'].read().decode(\"utf-8\"))\n",
    "    embeddings = response_json[\"embedding\"]\n",
    "    if len(embeddings) == 1:\n",
    "        return [embeddings[0]]\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d66036",
   "metadata": {},
   "source": [
    "### 7. Test the embeddings endpoint with a sample phrase\n",
    "Using any text phrase, the endpoint converts the text to a vectorized array of size 4096. We're also creating a function `embed_phrase` so that we can call it later. Running of the code cell below does not produce any output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8ab74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 9 ##\n",
    "\n",
    "def embed_phrase(input_data):\n",
    "    input_str = json.dumps({\"text_inputs\": input_data})\n",
    "    encoded_input_str = input_str.encode(\"utf-8\")\n",
    "    features = query_endpoint_with_json_payload(encoded_input_str,embedding_endpoint_name)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11bd3b3",
   "metadata": {},
   "source": [
    "Ask a general question about a book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e70458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 10 ##\n",
    "\n",
    "question_on_book=\"A book by the author edgar rice burroughs that is genre of science fiction and involves Tarzan the ape man\"\n",
    "result = embed_phrase(question_on_book)\n",
    "\n",
    "print(len(result[0]))\n",
    "result[0][:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaabc1e",
   "metadata": {},
   "source": [
    "### 8. Create an index in Amazon OpenSearch Service \n",
    "Whereas we previously created an index with 2-3 fields, this time we'll define the index with multiple fields: the vectorization of the `PlotSummary` field, and several others present within the dataset.\n",
    "\n",
    "To create the index, we first define the index in JSON, then use the aos_client connection we defined earlier to create the index in OpenSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba5754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 11 ##\n",
    "\n",
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "        \"index.knn.space_type\": \"cosinesimil\",\n",
    "        \"analysis\": {\n",
    "          \"analyzer\": {\n",
    "            \"default\": {\n",
    "              \"type\": \"standard\",\n",
    "              \"stopwords\": \"_english_\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"booksummary_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 4096,\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"book_summary\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"author\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"pub_date\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"genre\": {\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e84514d",
   "metadata": {},
   "source": [
    "Using the above index definition, we now need to create the index in Amazon OpenSearch. Running this cell will recreate the index if you have already executed this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464b0ac2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 12 ##\n",
    "\n",
    "index_name = \"book_knowledge_base\"\n",
    "\n",
    "try:\n",
    "    aos_client.indices.delete(index=index_name)\n",
    "    print(\"Recreating index '\" + index_name + \"' on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n",
    "except:\n",
    "    print(\"Index '\" + index_name + \"' not found. Creating index on cluster.\")\n",
    "    aos_client.indices.create(index=index_name,body=knn_index,ignore=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7007735",
   "metadata": {},
   "source": [
    "Let's verify the created index information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f71659d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 13 ##\n",
    "\n",
    "aos_client.indices.get(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040992c",
   "metadata": {},
   "source": [
    "### 9. Load the raw data into the Index\n",
    "Next, let's load the book summary data into the index we've just created. During the ingest data defined by the `os_import` function, the `book_summary` field will also be converted to vector (embedding) by calling the previously created endpoint using the embed_phrase function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55e6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 14 ##\n",
    "\n",
    "def os_import(record, aos_client, index_name):\n",
    "    book_summary = record[\"book_summary\"]\n",
    "    search_vector = embed_phrase(book_summary)\n",
    "    aos_client.index(index=index_name,\n",
    "             body={\"booksummary_vector\": search_vector[0], \n",
    "                   \"book_summary\": record[\"book_summary\"],\n",
    "                   \"author\":record[\"author\"],\n",
    "                   \"genre\":record[\"genre\"],\n",
    "                   \"pub_date\":record[\"pub_date\"],\n",
    "                   \"title\":record[\"title\"]\n",
    "                  }\n",
    "            )\n",
    "\n",
    "print(\"Loading records...\")\n",
    "for record in tqdm(wm_list): \n",
    "    os_import(record, aos_client, index_name)\n",
    "print(\"Records loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fad674",
   "metadata": {},
   "source": [
    "To validate the load, we'll query the number of documents number in the index. We should have approximately 4200 hits in the index, or however many was specified earlier in sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed0b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 15 ##\n",
    "\n",
    "res = aos_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "print(\"Records found: %d.\" % res['hits']['total']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b827c",
   "metadata": {},
   "source": [
    "### 10. Search vector with \"Semantic Search\" \n",
    "\n",
    "Now we can define a helper function to execute the search query for us to find a book whose review most closely matches the requested description. `retrieve_opensearch_with_semantic_search` embeds the search phrase, searches the index for the closest matching vector, and returns the top result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed4ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 16 ##\n",
    "\n",
    "def retrieve_opensearch_with_semantic_search(phrase, n=2):\n",
    "    \n",
    "    search_vector = embed_phrase(phrase)[0]\n",
    "\n",
    "    osquery={\n",
    "        \"_source\": {\n",
    "            \"exclude\": [ \"booksummary_vector\" ]\n",
    "        },\n",
    "        \n",
    "      \"size\": n,\n",
    "      \"query\": {\n",
    "        \"knn\": {\n",
    "          \"booksummary_vector\": {\n",
    "            \"vector\":search_vector,\n",
    "            \"k\":n\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "\n",
    "    res = aos_client.search(index=index_name, \n",
    "                           body=osquery,\n",
    "                           stored_fields=[\"title\",\"author\",\"pub_date\", \"genre\", \"book_summary\"],\n",
    "                           explain = True)\n",
    "    top_result = res['hits']['hits'][1]\n",
    "    \n",
    "    result = {\n",
    "        \"title\":top_result['_source']['title'],\n",
    "        \"author\":top_result['_source']['author'],\n",
    "        \"pub_date\":top_result['_source']['pub_date'],\n",
    "        \"genre\":top_result['_source']['genre'],\n",
    "        \"book_summary\":top_result['_source']['book_summary'],\n",
    "    }\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6820a",
   "metadata": {},
   "source": [
    "Use the semantic search to get similar records with the sample question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d529077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 17 ##\n",
    "\n",
    "example_request = retrieve_opensearch_with_semantic_search(question_on_book)\n",
    "print(question_on_book)\n",
    "print(example_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98987279",
   "metadata": {},
   "source": [
    "### 11. Get SageMaker endpoint for content generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6343640",
   "metadata": {},
   "source": [
    "We are using Falcon 7B LLM in this lab. Please refere HuggingFace documentaion for more information: https://huggingface.co/tiiuae/falcon-7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe808cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 18 ##\n",
    "\n",
    "llm_endpoint_name=outputs['LLMEndpointName']\n",
    "print(llm_endpoint_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd47ade",
   "metadata": {},
   "source": [
    "Define function to use LLM to generate content. As LLM is trained with static, outdated data, and it does not have business domain knowledge, the generated content is not factual(hallucination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40d5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 19 ##\n",
    "\n",
    "def query_llm_endpoint_with_json_payload(encoded_json, endpoint_name, content_type=\"application/json\"):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json\n",
    "    )\n",
    "    model_predictions = json.loads(response[\"Body\"].read())\n",
    "    return [gen[\"generated_text\"] for gen in model_predictions]\n",
    "\n",
    "def query_llm_with_hallucination(question):\n",
    "    payload = {\n",
    "        \"inputs\": question,\n",
    "        \"parameters\":{\n",
    "            \"max_new_tokens\": 1024,\n",
    "            \"num_return_sequences\": 1,\n",
    "            \"top_k\": 100,\n",
    "            \"top_p\": 0.95,\n",
    "            \"do_sample\": False,\n",
    "            \"return_full_text\": True,\n",
    "            \"temperature\": 0.9\n",
    "        }\n",
    "    }\n",
    "    query_response = query_llm_endpoint_with_json_payload(json.dumps(payload).encode(\"utf-8\"), endpoint_name=llm_endpoint_name)\n",
    "    return query_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb63ed8",
   "metadata": {},
   "source": [
    "Check the generated result from LLM by directly querying the invoke endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5542b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 20 ##\n",
    "\n",
    "generated_texts = query_llm_with_hallucination(question_on_book)\n",
    "\n",
    "print(f\"The recommended book from LLM without vector embedding: \\n\\n{generated_texts[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9e654",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation\n",
    "---\n",
    "To mitigate the LLM hallucination issue, we can provide more context to the LLM so that it can use the information to engineer a guiding prompt and generate more relevant results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa5564",
   "metadata": {},
   "source": [
    "### 12. Create a prompt for the LLM using the search results from OpenSearch\n",
    "\n",
    "We will be using the Falcon-7B model for one-shot generation, using a canned recommendation and response to guide the output. \n",
    "\n",
    "Before querying the model, the below function `generate_prompt_to_llm` is used to easily make a prompt for one-shot generation. The function takes in an input string to search the OpenSearch cluster for a matching book, then compose the prompt to LLM. The prompt is in the following format:\n",
    "\n",
    "```\n",
    "A librarian uses their vast knowledge of books to make great recommendations people will enjoy. As a librarian, you must include the author, the title of the book, and a summary of the plot relating to the following phrase: {original_question_on_book}.\n",
    "\n",
    "Data:{'book_summary': ' Tarzan tracks down a man who has been mistaken for him. The man is under the delusion that he is Tarzan, and he is living in a lost city inhabited by people descended from early Portuguese explorers. The plot devices of a lost city and a Tarzan \"double\" or impostor had been used by Burroughs in some previous Tarzan novels.', 'author': 'Edgar Rice Burroughs', 'title': 'Tarzan and the Madman', 'pub_date': 1964, 'genres': 'Science Fiction'}\n",
    "\n",
    "Recommendation:I have a wonderful book recommendation for you. It's a story about Tarzan and how a madman has been impersonating him. The author is Edgar Rice Burroughs and it's a science fiction book with adventure and fun. It was published in the year 1964. \n",
    "\n",
    "Data: {retrieved_documents}\n",
    "\n",
    "Recommendation:\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5367eacb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 21 ##\n",
    "\n",
    "def generate_prompt_to_llm(original_question_on_book):\n",
    "    retrieved_documents = retrieve_opensearch_with_semantic_search(original_question_on_book)\n",
    "    print(\"retrieved relevant book per your query is : \\n\" + str(retrieved_documents))\n",
    "    print(\"------------\")\n",
    "    one_shot_description_example = \"{'book_summary': 'Tarzan tracks down a man who has been mistaken for him. The man is under the delusion that he is Tarzan, and he is living in a lost city inhabited by people descended from early Portuguese explorers. The plot devices of a lost city and a Tarzan double or impostor had been used by Burroughs in some previous Tarzan novels.', 'author': 'Edgar Rice Burroughs', 'title': 'Tarzan and the Madman', 'genre': 'Science fiction', 'pub_date': '1964'}\"\n",
    "    one_shot_response_example = \"It's a real page turning story about Tarzan and how a madman has been impersonating him. The author is Edgar Rice Burroughs and it's a science fiction book with adventure and lots of fun. It was published in the year 1964.\"\n",
    "    prompt = (\n",
    "        f\" Make a book recommendation that is similar to the {original_question_on_book} The recommendation must include the title of the book, the author and genre: \\n\"\n",
    "        f\"Data: {one_shot_description_example} \\n Recommendation: {one_shot_response_example} \\n\"\n",
    "        f\"Data: {retrieved_documents} \\n Recommendation:\"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26aa75c",
   "metadata": {},
   "source": [
    "### 13. Format LLM prompt and query using the generated prompt\n",
    "We also need a few more helper functions to query the LLM. `generate_llm_input` transforms the generated prompt into the correct input format, `render_llm_output` parses the LLM output. \n",
    "\n",
    "`query_llm_with_rag` combines everything we've done in this module. It does all of the following:\n",
    "- generate vector for the input\n",
    "- searches the OpenSearch index with semantic search for the relevant book with \"description vector\"\n",
    "- generate an LLM prompt from the search results\n",
    "- queriy the LLM with RAG for a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073b9d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 22 ##\n",
    "\n",
    "def generate_llm_input(data, **kwargs):\n",
    "    default_kwargs = {\n",
    "        \"num_beams\": 5,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"temperature\": 0.9,\n",
    "        \"watermark\": True,\n",
    "        \"top_k\": 200,\n",
    "        \"max_length\": 200,\n",
    "        \"early_stopping\": True\n",
    "    }\n",
    "    \n",
    "    default_kwargs = {**default_kwargs, **kwargs}\n",
    "    \n",
    "    input_data = {\n",
    "        \"inputs\": data,\n",
    "        \"parameters\": default_kwargs\n",
    "    }\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "def query_llm_with_rag(description, **kwargs):\n",
    "    prompt = generate_prompt_to_llm(description)\n",
    "    query_payload = generate_llm_input(prompt, **kwargs)\n",
    "    response = query_llm_endpoint_with_json_payload(json.dumps(query_payload).encode(\"utf-8\"), endpoint_name=llm_endpoint_name)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62de7273",
   "metadata": {},
   "source": [
    "#### And finally, let's call the function and get a book recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccf971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Code Cell 23 ##\n",
    "\n",
    "recommendation = query_llm_with_rag(question_on_book)\n",
    "print(question_on_book)\n",
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46196052-ac1e-4d73-9218-6e49b35cc582",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 14. DIY Section\n",
    "\n",
    "To complete the solution you will need to make minor modifications to the code cells below to show your understanding of the concepts taught in this lab.\n",
    "\n",
    "- Clean up the existing dataframe by replacing all pub_date columns that are in YYYY-MM or YYYY to a default date of YYYY-MM-DD\n",
    "- Review the results that are obtained from the LLM by performing prompt engineering\n",
    "- Upload the resulting files to an S3 bucket for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240248c-b675-46bc-ae3b-b02b0387a1de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DIY Code Cell 1 ##\n",
    "\n",
    "# DIY: Modify the below two lines and replace the <column> place holder with the column that stores the book's published date.\n",
    "df_3.<column>.replace(r'^(\\d{4})$', r'\\1-01-01', regex=True,inplace=True)\n",
    "df_3.<column>.replace(r'^(\\d{4})-(\\d{2})$', r'\\1-\\2-01', regex=True,inplace=True)\n",
    "\n",
    "df_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89635038-5f5d-4947-b159-16bc5d15a0da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DIY Code Cell 2 ##\n",
    "\n",
    "question_on_book_diy = 'I read the following book ' + question_on_book + ' and hated it, can you recommend something different. Maybe a love story?'\n",
    "\n",
    "def generate_prompt_to_llm(original_question_on_book):\n",
    "    retrieved_documents = retrieve_opensearch_with_semantic_search(original_question_on_book)\n",
    "    print(\"retrieved relevant book per your query is : \\n\" + str(retrieved_documents))\n",
    "    print(\"------------\")\n",
    "    one_shot_description_example = \"{'book_summary': 'The novel opens during a Russian Orthodox funeral liturgy, or panikhida, for Yuri's mother, Marya Nikolaevna Zhivago. Having long ago been abandoned by his father, Yuri is taken in by his maternal uncle, a former Orthodox priest and philosopher. Formerly a wealthy member of Moscow's merchant gentry, Yuri's father, Andrei Zhivago, has squandered the family's millions through debauchery and carousing, and has been progressively bled dry by the corrupt lawyer Viktor', 'author': 'Boris Pasternak', 'title': 'Doctor Zhivago', 'genre': 'Novel', 'pub_date': '1957'}\"\n",
    "    one_shot_response_example = \"Here is a book that is very different from the one you read recently. It is not science fiction. Doctor Zhivago is a novel set in revolutionary russia during the 1900s, It is about Yuri Zhivago as he deals with revolution and social upheaval in his native country of russia.\"\n",
    "    prompt = (\n",
    "        f\" Make a book recommendation that is different than the author or genre as described here: {original_question_on_book} The output must include the title, the author and genre: \\n\"\n",
    "        f\"Data: {one_shot_description_example} \\n Recommendation: {one_shot_response_example} \\n\"\n",
    "        f\"Data: {retrieved_documents} \\n Recommendation:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "recommendation = query_llm_with_rag(question_on_book_diy)\n",
    "print(question_on_book_diy)\n",
    "print(recommendation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075f3b2-7596-4c1e-9689-1d03cc4ac367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DIY Code Cell 3 ##\n",
    "\n",
    "## NO NEED TO CHANGE ANY OF THIS CODE ##\n",
    "## RUN THIS CELL AS-IS TO COMPLETE THE DIY ##\n",
    "FILENAMES=['diy_out1.csv','diy_out2.txt']\n",
    "BUCKET='model-training'\n",
    "\n",
    "# Output the updated dataframe with correct pub_date to a csv file.\n",
    "df_3.to_csv(FILENAMES[0])\n",
    "\n",
    "# Output the new book recommendation to a text file.\n",
    "diy_file2 = open(FILENAMES[1], 'w')\n",
    "diy_file2.writelines(question_on_book_diy)\n",
    "diy_file2.writelines(recommendation)\n",
    "# Closing file\n",
    "diy_file2.close()\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.list_buckets()\n",
    "buckets = response['Buckets']\n",
    "\n",
    "# Find the correct bucket to upload the diy files to.\n",
    "bucket = [x['Name'] for x in buckets if BUCKET in x['Name']][0]\n",
    "\n",
    "for files in FILENAMES:\n",
    "    print(f\"Uploading file {files} to S3 bucket {bucket}\")\n",
    "    with open(files, \"rb\") as objects:\n",
    "        s3.upload_fileobj(objects, bucket, files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2e872-d4c1-4616-82da-7cd8406e68e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-cpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
